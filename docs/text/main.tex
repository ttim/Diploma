\chapter{Описание алгоритма}
\label{chap:main}

\section{Общая схема}
Поставим задачу классификации. Пространством объектов для классификации будет являться пространство $Tweets$. Множеством классов --- $C$. Также у нас есть обучающая и тестовая выборки.

Имея алгоритм классификации из пространства текстов ($Texts$) в множество $C$, можно получить и алгоритм классификации для пространства $Tweets$. Назовем данный алгоритм классификации (из текстов в классы) базовым алгоритмом $\mathrm{base\_classify} \colon Texts \rightarrow C$. Как было сказано на основе этого алгоритма можно построить алгоритм классификации записей из ``Твиттера'' $\mathrm{simple\_classify} \colon Tweets \rightarrow C$.

Пусть нам дан алгоритм кластеризации сообщений из ``Твиттера'' на какое-либо пространство $Y$ $\mathrm{clusterize} \colon Tweets \rightarrow Y$. Определим новый алгоритм классификации сообщений из Твиттера:\begin{gather*} 
\mathrm{context\_classify}(tweet) = \\ \mathop{\mathrm{argmax}}_C \ | \{ neighbor \in neighbors \colon \mathrm{clusterize}(neighbor) = \mathrm{clusterize}(tweet),\\ \mathrm{simple\_classify}(neighbor) = C \}|, 
\end{gather} где $neighbors$ --- это какие-либо сообщения автора исходного сообщения для классификации $tweet$. В данной работе такими сообщениями считаются последние $n$ сообщений автора. Говоря неформальным языком, мы будем относить новую запись к тому же классу, к которому отнесли весь кластер схожих записей того же автора.

Итак, менять характеристики этого алгоритма можно меняя базовый алгоритм класификации ($\mathrm{simple\_classify}$), алгоритм кластеризации ($\mathrm{clusterize}$) и модель выделения признаков из текста, параметр $n$.

\section{Модель текста на основе ``Википедии''}
\label{sec:wiki-model}
Пример выделения признаков из текста был указан в секции \ref{sec:text-model}. В данной секции мы хотели бы привести пример альтернативного метода, основанного на Википедии. Подход описанный в данной секции частично совпадает (в разделе нахождения релевантных статей) с подходом описанным в работе \cite{MEET:MEET14504801186}. В частности, выбор констант в формулах обусловлен выводами в статье.

Хотелось бы обратить внимание на мотивацию использования ``Википедии'' для выделения признаков из текста. При использовании стандартной модели текста, к примеру, для задачи кластеризации, два текста становятся похожими только в случае использования общих слов. В случае коротких текстов (таких как в микроблогах), такие совпадения редки, и нам бы хотелось понимать, что записи содержащие слово java и слово python, являются похожими друг на друга.

Опишем алгоритм выделения признаков из текста. Данный алгоритм будет состоять из двух шагов: выделения релевантных данному тексту статей в ``Википедии'' и построению признаков по данным статьям.

\subsection{Выделение релевантных статей}
Найдем статьи на которые есть ссылки по словам участвующим в тексте. Для каждой статьи подсчитаем два значения: linking probability и overlapping rate. Пусть, к примеру, статья у нас про язык ``Haskell''\footnote{http://ru.wikipedia.org/wiki/Haskell}, а слово --- ``Haskell''. Первое значение --- отношение количества ссылок ведущих на статью к общему количеству ссылок состоящих из тех же слов (в нашем примере это отношение количества ссылок в ``Википедии'' с текстом ``Haskell'' на страницу про язык ``Haskell'' к общему количеству ссылок с текстом ``Haskell''), второе значение --- размер пересечения текста записи и текста статьи из ``Википедии'' по отношению к размеру текста сообщения с учетом стемминга. Итоговым показателем (confidence) соответствия страницы из ``Википедии'' данному тексту будет среднее арифметическое полученных linking probability и overlapping rate, таким образом $0 \leqslant \mathrm{confidence} \leqslant 1$. Далее выбирается $n$ страниц с наибольшим confidence, но большим 0.3 согласно статье \cite{MEET:MEET14504801186}. Данные статьи и являются релевантными для данного текста.

\subsection{Выделение признаков по статьям}
Для каждой страницы в ``Википедии'' определены так называемые ``категории'' --- страницы с помощью которых организуются данные в ``Википедии''. К примеру, категориями страницы соответствующей языку программирования ``Haskell''\footnote{http://ru.wikipedia.org/wiki/Haskell}, являются ``Языки программирования по алфавиту'', ``Haskell'' и ``Языки программирования семейства Haskell''. В свою очередь для категорий определены над-категории --- категории содержащие данную категорию в качестве подкатегории. Категория ``Языки программирования семейства Haskell''\footnote{ru.wikipedia.org/wiki/Категория:Языки\_программирования\_семейства\_Haskell} является под-категорией следующих категорий: ``Семейства языков программирования'', ``Функциональные языки программирования''. Основываясь на этих данных можно определить следующие множества: \begin{gather*}
\mathrm{P}(category) = \text{все над-категории категории $category$}, \\
\mathrm{C}_{0}(page) = \text{все непосредственные категории страницы $page$}, \\
\mathrm{C}_{depth}(page) = \mathrm{C}_{depth-1}(page) \bigcup \Bigl\{\bigcup_{c \in \mathrm{C}_{depth-1}(page)} \mathrm{P}(c)\Bigr\}.
\end{gather*}

Зафиксируем какое-либо значение $max\_depth$. Теперь мы можем принять категории в ``Википедии'' в качестве бинарных признаков нашего текста (есть данная категория в множестве $\mathrm{C}_{max\_depth}(page)$ или её в нем нет). В данной работе $max\_depth$ было выбрано равным 4, так как такое значение по результатам работы \cite{article:max-depth} давало наилучший результат.
